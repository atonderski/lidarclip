{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import clip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "sys.path.append('..')\n",
    "from lidar_clippin.anno_loader import build_loader, CLASSES, WEATHERS\n",
    "from lidar_clippin.helpers import MultiLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use same prompts as prior art for objects\n",
    "OBJECT_PROMPT_TEMPLATES = [\n",
    "    'a bad photo of a {}.', 'a photo of many {}.', 'a sculpture of a {}.', 'a photo of the hard to see {}.', 'a low resolution photo of the {}.', 'a rendering of a {}.', 'graffiti of a {}.', 'a bad photo of the {}.', 'a cropped photo of the {}.', 'a tattoo of a {}.', 'the embroidered {}.', 'a photo of a hard to see {}.', 'a bright photo of a {}.', 'a photo of a clean {}.', 'a photo of a dirty {}.', 'a dark photo of the {}.', 'a drawing of a {}.', 'a photo of my {}.', 'the plastic {}.', 'a photo of the cool {}.', 'a close-up photo of a {}.', 'a black and white photo of the {}.', 'a painting of the {}.', 'a painting of a {}.', 'a pixelated photo of the {}.', 'a sculpture of the {}.', 'a bright photo of the {}.', 'a cropped photo of a {}.', 'a plastic {}.', 'a photo of the dirty {}.', 'a jpeg corrupted photo of a {}.', 'a blurry photo of the {}.', 'a photo of the {}.', 'a good photo of the {}.', 'a rendering of the {}.', 'a {} in a video game.', 'a photo of one {}.', 'a doodle of a {}.', 'a close-up photo of the {}.', 'a photo of a {}.', 'the origami {}.', 'the {} in a video game.', 'a sketch of a {}.', 'a doodle of the {}.', 'a origami {}.', 'a low resolution photo of a {}.', 'the toy {}.', 'a rendition of the {}.', 'a photo of the clean {}.', 'a photo of a large {}.', 'a rendition of a {}.', 'a photo of a nice {}.', 'a photo of a weird {}.', 'a blurry photo of a {}.', 'a cartoon {}.', 'art of a {}.', 'a sketch of the {}.', 'a embroidered {}.', 'a pixelated photo of a {}.', 'itap of the {}.', 'a jpeg corrupted photo of the {}.', 'a good photo of a {}.', 'a plushie {}.', 'a photo of the nice {}.', 'a photo of the small {}.', 'a photo of the weird {}.', 'the cartoon {}.', 'art of the {}.', 'a drawing of the {}.', 'a photo of the large {}.', 'a black and white photo of a {}.', 'the plushie {}.', 'a dark photo of a {}.', 'itap of a {}.', 'graffiti of the {}.', 'a toy {}.', 'itap of my {}.', 'a photo of a cool {}.', 'a photo of a small {}.', 'a tattoo of the {}.', 'there is a {} in the scene.', 'there is the {} in the scene.', 'this is a {} in the scene.', 'this is the {} in the scene.', 'this is one {} in the scene.',\n",
    "]\n",
    "\n",
    "# Generate weather and period prompts by permuting these attributes\n",
    "QUALITY_MODIFIERS = ['bad', 'good', 'clean', 'dirty', 'cropped', 'close-up']\n",
    "FORMAT_MODIFIERS = ['photo',]\n",
    "SCENE_MODIFIERS = ['environment', 'scene', 'road', 'street', 'intersection']\n",
    "CAPTURE_MODIFIERS = ['taken', 'captured',]\n",
    "\n",
    "WEATHER_PROMPT_TEMPLATES = set()\n",
    "for quality, format, scene, capture in itertools.product(QUALITY_MODIFIERS, FORMAT_MODIFIERS, SCENE_MODIFIERS, CAPTURE_MODIFIERS):\n",
    "    # Example: a good photo of a rainy environment\n",
    "    WEATHER_PROMPT_TEMPLATES.add(f'a {quality} {format} of a {{}} {scene}.')\n",
    "    WEATHER_PROMPT_TEMPLATES.add(f'a {quality} {format} {capture} on a {{}} day.')\n",
    "    WEATHER_PROMPT_TEMPLATES.add(f'a {quality} {format} {capture} in a {{}} {scene}.')\n",
    "    WEATHER_PROMPT_TEMPLATES.add(f'a {quality} {format} of many things in a {{}} {scene}.')\n",
    "\n",
    "PERIOD_PROMPT_TEMPLATES = set()\n",
    "for quality, format, scene, capture in itertools.product(QUALITY_MODIFIERS, FORMAT_MODIFIERS, SCENE_MODIFIERS, CAPTURE_MODIFIERS):\n",
    "    if quality == 'bright' or quality == 'dark':\n",
    "        continue  # Do not bias day/night by brightness in prompt\n",
    "    # Example: a good photo taken at night\n",
    "    PERIOD_PROMPT_TEMPLATES.add(f'a {quality} {format} {capture} at {{}}.')\n",
    "    # Example: a good photo of a scene taken at night\n",
    "    PERIOD_PROMPT_TEMPLATES.add(f'a {quality} {format} of a {scene} {capture} at {{}}.')\n",
    "    PERIOD_PROMPT_TEMPLATES.add(f'a {quality} {format} of many things in a {scene} {capture} at {{}}.')\n",
    "    PERIOD_PROMPT_TEMPLATES.add(f'a {quality} {format} of the {scene} {capture} at {{}}.')\n",
    "PERIOD_PROMPT_TEMPLATES = list(set(PERIOD_PROMPT_TEMPLATES))  # Remove duplicates\n",
    "\n",
    "EMPTY_PROMPTS = set()\n",
    "BUSY_PROMPTS = set()\n",
    "for quality, format, scene in itertools.product(QUALITY_MODIFIERS, FORMAT_MODIFIERS, SCENE_MODIFIERS):\n",
    "    # Example: a good photo of a busy environment\n",
    "    for busy_modifier in (\"busy\", \"crowded\", \"full\"):\n",
    "        BUSY_PROMPTS.add(f'a {quality} {format} of extremely {busy_modifier} traffic during rush hour with a large number of nearby vehicles.')\n",
    "\n",
    "    for empty_modifier in (\"empty\", \"deserted\", \"abandoned\"):\n",
    "        EMPTY_PROMPTS.add(f'a {quality} {format} of a completely {empty_modifier} {scene} with no vehicles in sight.')\n",
    "\n",
    "print(\"Num prompts per subcategory:\")\n",
    "print(f\"  Weather: {len(WEATHER_PROMPT_TEMPLATES)}\")\n",
    "print(f\"  Period: {len(PERIOD_PROMPT_TEMPLATES)}\")\n",
    "print(f\"  Busy: {len(BUSY_PROMPTS)}\")\n",
    "print(f\"  Empty: {len(EMPTY_PROMPTS)}\")\n",
    "print(f\"  Objects: {len(OBJECT_PROMPT_TEMPLATES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_VERSION = \"ViT-B/32\"\n",
    "# CLIP_VERSION = \"ViT-L/14\"\n",
    "USE_COSINE = False\n",
    "# USE_COSINE = True\n",
    "\n",
    "# Load data and features\n",
    "batch_size = 1\n",
    "clip_model, clip_preprocess = clip.load(CLIP_VERSION)\n",
    "dataset_root = \"/proj/nlp4adas/datasets/once\"\n",
    "dataset_root = \"/Users/s0000960/data/once/\"\n",
    "anno_loader = build_loader(dataset_root, clip_preprocess, batch_size=batch_size, num_workers=8, split=\"val\", skip_data=True, skip_anno=False)\n",
    "_val_loader = build_loader(dataset_root, clip_preprocess, batch_size=batch_size, num_workers=8, split=\"val\", skip_data=True, skip_anno=True)\n",
    "_test_loader = build_loader(dataset_root, clip_preprocess, batch_size=batch_size, num_workers=8, split=\"test\", skip_data=True, skip_anno=True)\n",
    "noanno_loader = MultiLoader([_val_loader, _test_loader])\n",
    "anno_dataset_for_vis, noanno_dataset_for_vis = None, None\n",
    "\n",
    "tmp = CLIP_VERSION.lower().replace(\"/\", \"-\")\n",
    "if USE_COSINE:\n",
    "    tmp += \"_cosine\"\n",
    "anno_img_feats = torch.load(f\"../features/once_{tmp}_val-anno_img.pt\").to(device)\n",
    "anno_lidar_feats = torch.load(f\"../features/once_{tmp}_val-anno_lidar.pt\").to(device)\n",
    "noanno_img_feats = torch.cat((torch.load(f\"../features/once_{tmp}_val_img.pt\"), torch.load(f\"../features/once_{tmp}_test_img.pt\")),dim=0).to(device)\n",
    "noanno_lidar_feats = torch.cat((torch.load(f\"../features/once_{tmp}_val_lidar.pt\"), torch.load(f\"../features/once_{tmp}_test_lidar.pt\")),dim=0).to(device)\n",
    "\n",
    "# Compute joint features\n",
    "# anno_joint_feats = anno_img_feats / anno_img_feats.norm(dim=1, keepdim=True) + anno_lidar_feats / anno_lidar_feats.norm(dim=1, keepdim=True)\n",
    "# noanno_joint_feats = noanno_img_feats / noanno_img_feats.norm(dim=1, keepdim=True) + noanno_lidar_feats / noanno_lidar_feats.norm(dim=1, keepdim=True)\n",
    "anno_joint_feats = anno_img_feats + anno_lidar_feats\n",
    "noanno_joint_feats = noanno_img_feats + noanno_lidar_feats\n",
    "\n",
    "assert noanno_img_feats.shape[0] == len(noanno_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build masks\n",
    "PERIODS = (\"night\", \"day\")\n",
    "WEATHERS = (\"sunny\", \"rainy\")\n",
    "BUSYNESS = (\"busy\", \"empty\")\n",
    "NEARBY_CLASSES = [f\"nearby {c}\" for c in CLASSES]\n",
    "\n",
    "masks = {name: torch.zeros(len(anno_loader), dtype=torch.bool) for name in itertools.chain(CLASSES, NEARBY_CLASSES, BUSYNESS)}\n",
    "masks.update({name: torch.zeros(len(noanno_loader), dtype=torch.bool) for name in itertools.chain(PERIODS, WEATHERS)})\n",
    "for i, (_, _, anno, meta) in tqdm(enumerate(noanno_loader)):\n",
    "    weather = meta[0][\"weather\"]\n",
    "    period = meta[0][\"period\"]\n",
    "    if period != \"night\":\n",
    "        period = \"day\"\n",
    "    if weather in WEATHERS:\n",
    "        masks[weather][i] = True\n",
    "    masks[period][i] = True\n",
    "\n",
    "for i, (_, _, anno, meta) in tqdm(enumerate(anno_loader)):\n",
    "    num_busy = 0\n",
    "    num_non_empty = 0\n",
    "    for name, box2d, box3d in zip(anno[0]['names'], anno[0]['boxes_2d'], anno[0]['boxes_3d']):\n",
    "        dist = torch.tensor(box3d[:3]).norm()\n",
    "        assert box2d[2] > box2d[0]\n",
    "        if dist < 15:\n",
    "            masks[f\"nearby {name}\"][i] = True\n",
    "        if name in (\"Car\", \"Bus\", \"Truck\"):\n",
    "            if dist < 15:\n",
    "                num_non_empty += 1\n",
    "            if dist < 40:\n",
    "                if box2d[2] - box2d[0] > 100:\n",
    "                    num_non_empty += 1\n",
    "                else:\n",
    "                    num_non_empty += 0.5\n",
    "            if dist < 60:\n",
    "                num_busy += 1\n",
    "        masks[name][i] = True\n",
    "    if num_busy >= 5:\n",
    "        masks[\"busy\"][i] = True\n",
    "    if num_non_empty < 1:\n",
    "        masks[\"empty\"][i] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Helper functions ###\n",
    "\n",
    "from typing import Callable, Iterable, List\n",
    "\n",
    "\n",
    "def logit_img_txt(img_feat, txt_feat, model):\n",
    "    img_feat = img_feat / img_feat.norm(dim=1, keepdim=True)\n",
    "    txt_feat = txt_feat / txt_feat.norm(dim=1, keepdim=True)\n",
    "\n",
    "    # cosine similarity as logits\n",
    "    logit_scale = model.logit_scale.exp().float()\n",
    "    logits_per_image = logit_scale * img_feat.float() @ txt_feat.t().float()\n",
    "    logits_per_text = logits_per_image.t()\n",
    "    return logits_per_text, logits_per_image\n",
    "\n",
    "def get_topk(prompts, k, img_feats, lidar_feats, joint_feats):\n",
    "    text = clip.tokenize(prompts).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text)\n",
    "        text_features = text_features.sum(axis=0, keepdim=True)\n",
    "    logits_per_text_i, logits_per_img = logit_img_txt(img_feats, text_features, clip_model)\n",
    "    logits_per_text_l, logits_per_pc = logit_img_txt(lidar_feats, text_features, clip_model)\n",
    "    logits_per_text_j, logits_per_joint = logit_img_txt(joint_feats, text_features, clip_model)\n",
    "    # logits_per_text_j = logits_per_text_i + logits_per_text_l\n",
    "\n",
    "    _, img_idxs = torch.topk(logits_per_text_i[0,:], k)\n",
    "    _, pc_idxs = torch.topk(logits_per_text_l[0,:], k)\n",
    "    _, joint_idxs = torch.topk(logits_per_text_j[0,:], k)\n",
    "\n",
    "    # Rank separately and then fuse rankings\n",
    "    # pc_ranking = torch.argsort(torch.argsort(logits_per_text_l[0,:]))\n",
    "    # img_ranking = torch.argsort(torch.argsort(logits_per_text_i[0,:]))\n",
    "    # joint_ranking = pc_ranking * img_ranking\n",
    "    # _, joint_idxs = torch.topk(joint_ranking, k)\n",
    "\n",
    "    # Perform reranking. first round selects top 1% of the candidates, second round selects top K remaining candidates\n",
    "    # second_round_scores = -torch.ones(lidar_feats.shape[0])\n",
    "    # _, first_rank_winners = torch.topk(logits_per_text_i[0,:], 100*k)\n",
    "    # second_round_scores[first_rank_winners] = logits_per_text_l[0,first_rank_winners]\n",
    "    # _, first_rank_winners = torch.topk(logits_per_text_l[0,:], 100*k)\n",
    "    # second_round_scores[first_rank_winners] = logits_per_text_i[0,first_rank_winners]\n",
    "    # _, joint_idxs = torch.topk(second_round_scores, k)\n",
    "\n",
    "    return img_idxs.numpy(), pc_idxs.numpy(), joint_idxs.numpy()\n",
    "\n",
    "\n",
    "def get_topk_separate_prompts(image_prompts, lidar_prompts, k, img_feats, lidar_feats):\n",
    "    with torch.no_grad():\n",
    "        text_features_image = clip_model.encode_text(clip.tokenize(image_prompts).to(device)).sum(axis=0, keepdim=True)\n",
    "        text_features_lidar = clip_model.encode_text(clip.tokenize(lidar_prompts).to(device)).sum(axis=0, keepdim=True)\n",
    "    logits_per_text_i, logits_per_img = logit_img_txt(img_feats, text_features_image, clip_model)\n",
    "    logits_per_text_l, logits_per_pc = logit_img_txt(lidar_feats, text_features_lidar, clip_model)\n",
    "    logits_per_text_j = logits_per_text_i + logits_per_text_l\n",
    "\n",
    "    _, pc_idxs = torch.topk(logits_per_text_l[0,:], k)\n",
    "    _, img_idxs = torch.topk(logits_per_text_i[0,:], k)\n",
    "    _, joint_idxs = torch.topk(logits_per_text_j[0,:], k)\n",
    "\n",
    "    return img_idxs.numpy(), pc_idxs.numpy(), joint_idxs.numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "VERBOSE = False\n",
    "USE_ANNOS = True\n",
    "# Computes overall score average for all object categories\n",
    "image_overall, lidar_overall, joint_overall = 0.0, 0.0, 0.0\n",
    "\n",
    "subcategories = [(PERIODS, PERIOD_PROMPT_TEMPLATES, False), (WEATHERS, WEATHER_PROMPT_TEMPLATES, False)]\n",
    "if USE_ANNOS:\n",
    "    subcategories += [\n",
    "        (CLASSES, OBJECT_PROMPT_TEMPLATES, True),\n",
    "        (NEARBY_CLASSES, OBJECT_PROMPT_TEMPLATES, True),\n",
    "        (BUSYNESS, {\"busy\": BUSY_PROMPTS, \"empty\": EMPTY_PROMPTS}, True)\n",
    "    ]\n",
    "total_num = sum(len(subcat[0]) for subcat in subcategories)\n",
    "\n",
    "image_overall, lidar_overall, joint_overall = 0.0, 0.0, 0.0\n",
    "for subcategory, prompt_template, use_annos in subcategories:\n",
    "    image_subcat, lidar_subcat, joint_subcat = 0.0, 0.0, 0.0\n",
    "    for name in subcategory:\n",
    "        if isinstance(prompt_template, dict):\n",
    "            prompts = prompt_template[name]\n",
    "        else:\n",
    "            prompts = [prompt.format(name) for prompt in prompt_template]\n",
    "        if use_annos:\n",
    "            img_idxs, pc_idxs, joint_idxs = get_topk(prompts, K, anno_img_feats, anno_lidar_feats, anno_joint_feats)\n",
    "        else:\n",
    "            img_idxs, pc_idxs, joint_idxs = get_topk(prompts, K, noanno_img_feats, noanno_lidar_feats, noanno_joint_feats)\n",
    "        num_positives = masks[name].sum().item()\n",
    "        best_case = min(num_positives, K)\n",
    "        image_score = masks[name][img_idxs].sum().numpy()/best_case\n",
    "        lidar_score = masks[name][pc_idxs].sum().numpy()/best_case\n",
    "        joint_score = masks[name][joint_idxs].sum().numpy()/best_case\n",
    "        # random_score = masks[name][random_idxs].sum().numpy()/best_case\n",
    "        if VERBOSE:\n",
    "            print(f\"R@{K} for {name}: ({num_positives} matches)\")\n",
    "            print(\"    image: \", image_score)\n",
    "            print(\"    lidar: \", lidar_score)\n",
    "            print(\"    joint: \", joint_score)\n",
    "            # print(\"    random:\", random_score)\n",
    "        image_subcat += image_score/len(subcategory)\n",
    "        lidar_subcat += lidar_score/len(subcategory)\n",
    "        joint_subcat += joint_score/len(subcategory)\n",
    "        image_overall += image_score/total_num\n",
    "        lidar_overall += lidar_score/total_num\n",
    "        joint_overall += joint_score/total_num\n",
    "    if VERBOSE:\n",
    "        print(f\"Average R@{K} for {subcategory}:\")\n",
    "        print(\"    image: \", image_subcat)\n",
    "        print(\"    lidar: \", lidar_subcat)\n",
    "        print(\"    joint: \", joint_subcat)\n",
    "        print(\"=========================================\")\n",
    "print(f\"R@{K} overall:\")\n",
    "print(\"    image: \", image_overall)\n",
    "print(\"    lidar: \", lidar_overall)\n",
    "print(\"    joint: \", joint_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subcategory, prompt_template, use_annos in subcategories:\n",
    "    for name in subcategory:\n",
    "        num_positives = masks[name].sum().item()\n",
    "        print(f\"random score for {name}: {num_positives/len(noanno_lidar_feats):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if anno_dataset_for_vis is None or noanno_dataset_for_vis is None:\n",
    "    split = \"val\"\n",
    "    use_annos = True\n",
    "    anno_dataset_for_vis = build_loader(dataset_root, clip_preprocess, batch_size=batch_size, num_workers=1, split=split, skip_data=False, skip_anno=not use_annos).dataset\n",
    "    noanno_dataset_for_vis = MultiLoader([\n",
    "        build_loader(dataset_root, clip_preprocess, batch_size=1, num_workers=1, split=\"val\", skip_data=False, skip_anno=True),\n",
    "        build_loader(dataset_root, clip_preprocess, batch_size=1, num_workers=1, split=\"test\", skip_data=False, skip_anno=True),\n",
    "    ])\n",
    "means = torch.tensor([0.48145466, 0.4578275, 0.40821073], device=\"cpu\")\n",
    "stds = torch.tensor([0.26862954, 0.26130258, 0.27577711], device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K2 = 6\n",
    "# img_idxs, pc_idxs, joint_idxs = get_topk(EMPTY_PROMPTS, K2, anno_img_feats, anno_lidar_feats, anno_joint_feats)\n",
    "# image_prompts = [\"a photo with large water droplets on the lens\", \"an extremely blurry photo where no details can be discerned\", \"an extremely rainy scene with huge water sprays\"]\n",
    "image = [\"a nearby semi truck\", \"a semi truck in the middle of the road\", \"a semi truck on the street\"]\n",
    "# image_prompts = [\"a photo with extreme glare/\", \"an extremely corrupted photo where no details can be discerned\", \"an extremely blinded and glared photo\", \"a corrupted photo with extreme glare and no objects can be seen\"]\n",
    "# lidar_prompts = [\"a nearby car\", \"a car in the middle of the road\", \"a car on the street\"]\n",
    "# image_prompts = [\"a nearby pedestrian\", \"a semi large number of nearby people\", \"a person in the middle of the road\", \"a person on the street\"]\n",
    "# image_prompts = [\"a photo of a dog\", \"animal\", \"pet\", \"person walking his dog\", \"dog crossing the street\"]\n",
    "# image_prompts = [\"a photo of a nearby tuk-tuk, a type of three-wheeler\", \"a three-wheeler in the middle of the road\", \"a tuk-tuk, three-wheeler, on the street\"]\n",
    "# image_prompts = [p.format(\"night\") for p in PERIOD_PROMPT_TEMPLATES]\n",
    "# image_prompts = [p.format(\"foggy not wet\") for p in WEATHER_PROMPT_TEMPLATES]\n",
    "image_prompts = [\"a small child on the sidewalk\", \"a kid walking with his parents\", \"a very tiny person\"]\n",
    "lidar_prompts = image_prompts\n",
    "SUBSAMPLING = 1  # avoid picking extremely similar samples\n",
    "OFFSET = 0\n",
    "img_idxs, pc_idxs, joint_idxs = get_topk_separate_prompts(image_prompts, lidar_prompts, K2, noanno_img_feats[OFFSET::SUBSAMPLING], noanno_lidar_feats[OFFSET::SUBSAMPLING])\n",
    "dataset_for_vis = noanno_dataset_for_vis\n",
    "pc_idxs = pc_idxs * SUBSAMPLING + OFFSET\n",
    "img_idxs = img_idxs * SUBSAMPLING + OFFSET\n",
    "joint_idxs = joint_idxs * SUBSAMPLING + OFFSET\n",
    "\n",
    "PLOT_IMAGE = True\n",
    "PLOT_LIDAR = True\n",
    "PLOT_JOINT = False\n",
    "\n",
    "rows = int(PLOT_IMAGE) + int(PLOT_LIDAR) + int(PLOT_JOINT)\n",
    "\n",
    "fig = plt.figure(figsize=(10*K2,10*rows), dpi=200)\n",
    "width, height = 1/K2, 1/(rows+0.1)\n",
    "# ax = fig.add_axes([0, 0, 1, 1])\n",
    "\n",
    "curr_row = 1\n",
    "if PLOT_IMAGE:\n",
    "    for i, idx in enumerate(img_idxs):\n",
    "        ax = fig.add_axes([i/K2, 1/rows, width, height])\n",
    "        ax.imshow((dataset_for_vis[idx][0].permute(1,2,0)*stds + means).numpy())\n",
    "        ax.axis('off')\n",
    "    curr_row+=1\n",
    "\n",
    "if PLOT_LIDAR:\n",
    "    for i, idx in enumerate(pc_idxs):\n",
    "        ax = fig.add_axes([i/K2, curr_row/rows, width, height])\n",
    "        ax.imshow((dataset_for_vis[idx][0].permute(1,2,0)*stds + means).numpy())\n",
    "        ax.axis('off')\n",
    "\n",
    "    curr_row+=1\n",
    "\n",
    "if PLOT_JOINT:\n",
    "    for i, idx in enumerate(joint_idxs):\n",
    "        img, pc = dataset_for_vis[idx][:2]\n",
    "        # WHAT THE FUCK IS GOING ON HERE. I HAVE TO DO THIS TO GET THE RIGHT ORDERING\n",
    "        ax = fig.add_axes([i/K2, 0, width, height])\n",
    "        ax.imshow((img.permute(1,2,0)*stds + means).numpy())\n",
    "        ax.axis('off')\n",
    "        \n",
    "        ax = fig.add_axes([i/K2 + 1.96/K2/3, 1.93/3/rows, width/3, height/3])\n",
    "        # draw a white box around the image\n",
    "        rect = patches.Rectangle((-10,4), 20, 16, alpha=1.0, facecolor='white')\n",
    "        ax.add_patch(rect)\n",
    "        # draw the point cloud\n",
    "        pc = pc[pc[:,0] < 20]\n",
    "        pc = pc[pc[:,1] < 10]\n",
    "        pc = pc[pc[:,1] > -10]\n",
    "        col = pc[:,3]\n",
    "        ax.scatter(-pc[:,1], pc[:,0], s=0.1, c=col**0.3, cmap=\"coolwarm\")\n",
    "        ax.axis(\"scaled\")\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_ylim(0, 20)\n",
    "        ax.set_xlim(-10, 10)\n",
    "    curr_row+=1\n",
    "\n",
    "print(pc_idxs)\n",
    "print(img_idxs)\n",
    "print(joint_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, idx in enumerate(pc_idxs):\n",
    "    plt.imshow((dataset_for_vis[idx][0].permute(1,2,0)*stds + means).numpy())\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"headlights_{}.png\".format(i), bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero_shot_classes =  [\"car\", \"bus\", \"truck\", \"person\", \"bike or moped\",  \"animal\", \"tree or bush\", \"three-wheeler\"]\n",
    "zero_shot_classes =  [\"car\", \"bus\", \"truck\", \"person\", \"two-wheeler\",  \"animal\", \"three-wheeler\"]\n",
    "# reverse the order to make plots look good hehe\n",
    "zero_shot_classes.reverse()\n",
    "class_embeddings = []\n",
    "for cls_name in tqdm(zero_shot_classes, \"computing class embeddings...\"):\n",
    "    # print(\"embedding \", cls_name)\n",
    "    prompts = [template.format(cls_name) for template in OBJECT_PROMPT_TEMPLATES]\n",
    "    with torch.no_grad():\n",
    "        class_embeddings.append(clip_model.encode_text(clip.tokenize(prompts).to(device)).sum(axis=0, keepdim=True))\n",
    "class_embeddings = torch.cat(class_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE_IDXS = [\n",
    "#     61687,  # greenery\n",
    "#     0,      # bus\n",
    "#     1947,   # car\n",
    "#     8601,   # bus-car mix\n",
    "#     88888,  # nearby car\n",
    "#     58777,  # person biking\n",
    "#     113859, # moped\n",
    "#     13406,  # person walking unclear object\n",
    "#     96150,  # person with umbrella\n",
    "#     34170,  # person with backpack\n",
    "#     58854,  # doggo\n",
    "# ]\n",
    "SAMPLE_IDXS = [109737]\n",
    "dataset_for_vis = noanno_dataset_for_vis\n",
    "lidar_feat = noanno_lidar_feats[SAMPLE_IDXS]\n",
    "image_feat = noanno_img_feats[SAMPLE_IDXS]\n",
    "joint_feat = noanno_joint_feats[SAMPLE_IDXS]\n",
    "with torch.no_grad():\n",
    "    lidar_scores_all = logit_img_txt(lidar_feat, class_embeddings, clip_model)[0].softmax(0)\n",
    "    image_scores_all = logit_img_txt(image_feat, class_embeddings, clip_model)[0].softmax(0)\n",
    "    joint_scores_all = logit_img_txt(joint_feat, class_embeddings, clip_model)[0].softmax(0)\n",
    "\n",
    "for i, sample_idx in enumerate(SAMPLE_IDXS):\n",
    "    img, pc = dataset_for_vis[sample_idx][:2]\n",
    "    lidar_scores = lidar_scores_all[:, i]\n",
    "    image_scores = image_scores_all[:, i]\n",
    "    joint_scores = joint_scores_all[:, i]\n",
    "\n",
    "    fig = plt.figure(figsize=(7,5), dpi=150)\n",
    "\n",
    "    # Draw the image\n",
    "    ax = fig.add_axes([0, 0, 5/7, 1])\n",
    "    ax.imshow((img.permute(1,2,0)*stds + means).numpy())\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Draw the point cloud\n",
    "    ax = fig.add_axes([0.455, 0.698, 0.3, 0.3])\n",
    "    rect = patches.Rectangle((-10,0), 20, 20, alpha=1.0, facecolor='white')\n",
    "    ax.add_patch(rect)\n",
    "    pc = pc[pc[:,0] < 20]\n",
    "    pc = pc[pc[:,1] < 10]\n",
    "    pc = pc[pc[:,1] > -10]\n",
    "    col = pc[:,3]\n",
    "    ax.scatter(-pc[:,1], pc[:,0], s=0.1, c=col**0.3, cmap=\"coolwarm\")\n",
    "    ax.axis(\"scaled\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_ylim(0, 20)\n",
    "    ax.set_xlim(-10, 10)\n",
    "\n",
    "    # Draw the class bar chart\n",
    "    ax = fig.add_axes([5/7+0.001, 0.05, 2/7-0.001, 0.9])\n",
    "    # plot class scores as a horizontal bar chart with image lidar and joint scores side by side\n",
    "    # top down not down up\n",
    "    ax.barh(np.arange(len(zero_shot_classes)) + 0.34, image_scores, height=0.15, color='#8DD376', label='image')\n",
    "    ax.barh(np.arange(len(zero_shot_classes)) + 0.17, lidar_scores, height=0.15, color='#DE8B8A', label='lidar')\n",
    "    ax.barh(np.arange(len(zero_shot_classes)) , joint_scores, height=0.15, color='#B172E0', label='joint')\n",
    "    ax.set_yticks(np.arange(len(zero_shot_classes))+0.57)\n",
    "    ax.set_yticklabels(zero_shot_classes)\n",
    "    ax.set_xticks([0.1, 0.4, 0.7, 1.0])\n",
    "    ax.yaxis.tick_right()\n",
    "    ax.tick_params(axis=\"y\", direction=\"in\", pad=-142, labelsize=10, length=0)\n",
    "    ax.tick_params(axis=\"x\", direction='inout', length=5)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.spines.right.set_visible(False)\n",
    "    ax.spines.left.set_visible(False) \n",
    "    ax.spines.top.set_visible(False)\n",
    "    ax.set_title(\"Classification scores\")\n",
    "    ax.legend()\n",
    "    fig.savefig(\"{}.png\".format(sample_idx), bbox_inches='tight', pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_scores_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_TO_SAVE = 109737\n",
    "tmp = \"ViT-B/32\".lower().replace(\"/\", \"-\")\n",
    "save_feat = torch.cat((torch.load(f\"../features/once_{tmp}_val_lidar.pt\"), torch.load(f\"../features/once_{tmp}_test_lidar.pt\")),dim=0)[ID_TO_SAVE]\n",
    "torch.save(save_feat.clone(), f\"lidar_feat_{ID_TO_SAVE}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(dataset_for_vis[ID_TO_SAVE][0].permute(1,2,0)*stds + means)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"front_page_image.png\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5), dpi=200)\n",
    "\n",
    "# Draw the point cloud\n",
    "pc  = dataset_for_vis[ID_TO_SAVE][1].numpy()\n",
    "pc = pc[pc[:,0] < 20]\n",
    "pc = pc[pc[:,1] < 10]\n",
    "pc = pc[pc[:,1] > -10]\n",
    "col = pc[:,3]\n",
    "plt.scatter(-pc[:,1], pc[:,0], s=0.1, c=col**0.3, cmap=\"coolwarm\")\n",
    "plt.axis(\"scaled\")\n",
    "plt.axis(\"off\")\n",
    "plt.ylim(0, 20)\n",
    "plt.xlim(-10, 10)\n",
    "plt.savefig(\"front_page_lidar.png\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lidar_scores)\n",
    "print(zero_shot_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('lidarclip')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8474fbf7fa6f299d9ca87dcd7358dfc28aa95d8ec78802489d98a6cd3ecc0cc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
