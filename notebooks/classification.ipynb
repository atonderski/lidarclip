{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import itertools\n",
    "from functools import partial\n",
    "from typing import Dict\n",
    "\n",
    "from tqdm import tqdm\n",
    "import clip\n",
    "sys.path.append('..')\n",
    "from lidarclip.anno_loader import build_anno_loader, CLASSES, WEATHERS\n",
    "from lidarclip.helpers import MultiLoader, try_paths, logit_img_txt\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num prompts per subcategory:\n",
      "  Objects: 85\n"
     ]
    }
   ],
   "source": [
    "from lidarclip.prompts import OBJECT_PROMPT_TEMPLATES\n",
    "print(\"Num prompts per subcategory:\")\n",
    "print(f\"  Objects: {len(OBJECT_PROMPT_TEMPLATES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car 44829\n",
      "torch.Size([44829, 768])\n",
      "Bus 2617\n",
      "torch.Size([2617, 768])\n",
      "Truck 1672\n",
      "torch.Size([1672, 768])\n",
      "Cyclist 11894\n",
      "torch.Size([11894, 768])\n",
      "Pedestrian 17930\n",
      "torch.Size([17930, 768])\n"
     ]
    }
   ],
   "source": [
    "CLIP_VERSION = \"ViT-L/14\"\n",
    "\n",
    "# Load data and features\n",
    "batch_size = 1\n",
    "clip_model, clip_preprocess = clip.load(CLIP_VERSION)\n",
    "feature_version = CLIP_VERSION.lower().replace(\"/\", \"-\")\n",
    "feature_root = try_paths(\"/proj/nlp4adas/features\", \"../features\")\n",
    "obj_feats = torch.load(f\"{feature_root}/once_{feature_version}_val_lidar_objs.pt\", map_location=device)\n",
    "for class_name, cls_feats in obj_feats.items():\n",
    "    print(class_name, len(cls_feats))\n",
    "    obj_feats[class_name] = torch.stack(cls_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for Car\n",
      "Generating embedding for Truck\n",
      "Generating embedding for Bus\n",
      "Generating embedding for Pedestrian\n",
      "Generating embedding for Cyclist\n",
      "Generated embeddings for:  ['Car', 'Truck', 'Bus', 'Pedestrian', 'Cyclist']\n"
     ]
    }
   ],
   "source": [
    "CATEGORIES = CLASSES\n",
    "def gen_cls_embedding(cls_name: str) -> torch.Tensor:\n",
    "    print(f\"Generating embedding for {cls_name}\")\n",
    "    # prompts = [template.format(cls_name) for template in OBJECT_PROMPT_TEMPLATES]\n",
    "    prompts = [cls_name]\n",
    "    with torch.no_grad():\n",
    "        tokenized_prompts = clip.tokenize(prompts).to(device)\n",
    "        cls_features = clip_model.encode_text(tokenized_prompts)\n",
    "        return cls_features.sum(axis=0, keepdim=True)\n",
    "cls_embeddings = {name: gen_cls_embedding(name) for name in CATEGORIES}\n",
    "print(\"Generated embeddings for: \", list(cls_embeddings.keys()))\n",
    "cls_embeddings_pt = torch.vstack(list(cls_embeddings.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating class Car (n=44829)\n",
      "  top-1: 0.026 (2.6%), top-2: 0.244 (24.4%), top-3: 0.986 (98.6%), top-4: 0.999 (99.9%), top-5: 1.000 (100.0%)\n",
      "Evaluating class Bus (n=2617)\n",
      "  top-1: 0.006 (0.6%), top-2: 0.021 (2.1%), top-3: 0.043 (4.3%), top-4: 0.084 (8.4%), top-5: 1.000 (100.0%)\n",
      "Evaluating class Truck (n=1672)\n",
      "  top-1: 0.010 (1.0%), top-2: 0.035 (3.5%), top-3: 0.076 (7.6%), top-4: 0.989 (98.9%), top-5: 1.000 (100.0%)\n",
      "Evaluating class Cyclist (n=11894)\n",
      "  top-1: 0.176 (17.6%), top-2: 0.691 (69.1%), top-3: 0.935 (93.5%), top-4: 0.989 (98.9%), top-5: 1.000 (100.0%)\n",
      "Evaluating class Pedestrian (n=17930)\n",
      "  top-1: 0.618 (61.8%), top-2: 0.896 (89.6%), top-3: 0.961 (96.1%), top-4: 0.994 (99.4%), top-5: 1.000 (100.0%)\n",
      "\n",
      "Overall:\n",
      "  top-1: 0.167 (16.7%), top-2: 0.377 (37.7%), top-3: 0.600 (60.0%), top-4: 0.811 (81.1%), top-5: 1.000 (100.0%)\n",
      "\n",
      "Overall if guessing randomly:\n",
      "  top-1: 0.200 (20.0%), top-2: 0.400 (40.0%), top-3: 0.600 (60.0%), top-4: 0.800 (80.0%), top-5: 1.000 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def compute_accuracy(obj_feats: torch.Tensor, tru_class_idx: int) -> Dict[str, float]:\n",
    "    logits_per_text, _ = logit_img_txt(obj_feats, cls_embeddings_pt, clip_model)\n",
    "    score_per_class = logits_per_text.softmax(0).T\n",
    "    accuracies = {}\n",
    "    for k in range(1, min(6, len(score_per_class))):\n",
    "        topk = (score_per_class.argsort(axis=1, descending=True)[:, :k] == tru_class_idx).sum() / len(score_per_class)\n",
    "        accuracies[f\"top-{k}\"] = topk\n",
    "    return accuracies\n",
    "\n",
    "overall = defaultdict(float)\n",
    "for class_name, cls_obj_feats in obj_feats.items():\n",
    "    print(\"Evaluating class\", class_name, f\"(n={len(cls_obj_feats)})\")\n",
    "    accuracies = compute_accuracy(cls_obj_feats, CATEGORIES.index(class_name))\n",
    "    res_string = \", \".join(f\"{k}: {v:.3f} ({v*100:.1f}%)\" for k, v in accuracies.items())\n",
    "    print(f\"  {res_string}\")\n",
    "    for k, v in accuracies.items():\n",
    "        overall[k] += v\n",
    "overall = {k: v / len(obj_feats) for k, v in overall.items()}\n",
    "res_string = \", \".join(f\"{k}: {v:.3f} ({v*100:.1f}%)\" for k, v in overall.items())\n",
    "print(f\"\\nOverall:\")\n",
    "print(f\"  {res_string}\")\n",
    "print(\"\\nOverall if guessing randomly:\")\n",
    "# Compute the probability of guessing correctly by chance\n",
    "rand_acc = 1 / len(CATEGORIES)\n",
    "res_string = \", \".join(f\"{k}: {v:.3f} ({v*100:.1f}%)\" for k, v in {k: rand_acc*int(k.split(\"-\")[1]) for k in overall}.items())\n",
    "print(f\"  {res_string}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8474fbf7fa6f299d9ca87dcd7358dfc28aa95d8ec78802489d98a6cd3ecc0cc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
