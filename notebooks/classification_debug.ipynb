{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import itertools\n",
    "from functools import partial\n",
    "from typing import Dict\n",
    "\n",
    "from tqdm import tqdm\n",
    "import clip\n",
    "sys.path.append('..')\n",
    "from lidarclip.anno_loader import build_anno_loader, CLASSES, WEATHERS\n",
    "from lidarclip.helpers import MultiLoader, try_paths, logit_img_txt\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num prompts per subcategory:\n",
      "  Objects: 85\n"
     ]
    }
   ],
   "source": [
    "from lidarclip.prompts import OBJECT_PROMPT_TEMPLATES\n",
    "print(\"Num prompts per subcategory:\")\n",
    "print(f\"  Objects: {len(OBJECT_PROMPT_TEMPLATES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../features/once_vit-l-14_val_lidar_objs_debug_bev.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m feature_version \u001b[39m=\u001b[39m CLIP_VERSION\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m feature_root \u001b[39m=\u001b[39m try_paths(\u001b[39m\"\u001b[39m\u001b[39m/proj/nlp4adas/features\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m../features\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m bev_feats \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mfeature_root\u001b[39m}\u001b[39;49;00m\u001b[39m/once_\u001b[39;49m\u001b[39m{\u001b[39;49;00mfeature_version\u001b[39m}\u001b[39;49;00m\u001b[39m_val_lidar_objs_debug_bev.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m, map_location\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(bev_feats\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lidarclip/lib/python3.9/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lidarclip/lib/python3.9/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lidarclip/lib/python3.9/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../features/once_vit-l-14_val_lidar_objs_debug_bev.pt'"
     ]
    }
   ],
   "source": [
    "CLIP_VERSION = \"ViT-L/14\"\n",
    "\n",
    "# Load data and features\n",
    "batch_size = 1\n",
    "clip_model, clip_preprocess = clip.load(CLIP_VERSION)\n",
    "feature_version = CLIP_VERSION.lower().replace(\"/\", \"-\")\n",
    "feature_root = try_paths(\"/proj/nlp4adas/features\", \"../features\")\n",
    "bev_feats = torch.load(f\"{feature_root}/once_{feature_version}_val_lidar_objs_debug_bev.pt\", map_location=device)\n",
    "print(bev_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for Car\n",
      "Generating embedding for Truck\n",
      "Generating embedding for Bus\n",
      "Generating embedding for Pedestrian\n",
      "Generating embedding for Cyclist\n",
      "Generated embeddings for:  ['Car', 'Truck', 'Bus', 'Pedestrian', 'Cyclist']\n"
     ]
    }
   ],
   "source": [
    "CATEGORIES = CLASSES\n",
    "def gen_cls_embedding(cls_name: str) -> torch.Tensor:\n",
    "    print(f\"Generating embedding for {cls_name}\")\n",
    "    # prompts = [template.format(cls_name) for template in OBJECT_PROMPT_TEMPLATES]\n",
    "    prompts = [cls_name]\n",
    "    with torch.no_grad():\n",
    "        tokenized_prompts = clip.tokenize(prompts).to(device)\n",
    "        cls_features = clip_model.encode_text(tokenized_prompts)\n",
    "        return cls_features.sum(axis=0, keepdim=True)\n",
    "cls_embeddings = {name: gen_cls_embedding(name) for name in CATEGORIES}\n",
    "print(\"Generated embeddings for: \", list(cls_embeddings.keys()))\n",
    "cls_embeddings_pt = torch.vstack(list(cls_embeddings.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating class Car (n=44829)\n",
      "  top-1: 0.026 (2.6%), top-2: 0.244 (24.4%), top-3: 0.986 (98.6%), top-4: 0.999 (99.9%), top-5: 1.000 (100.0%)\n",
      "Evaluating class Bus (n=2617)\n",
      "  top-1: 0.006 (0.6%), top-2: 0.021 (2.1%), top-3: 0.043 (4.3%), top-4: 0.084 (8.4%), top-5: 1.000 (100.0%)\n",
      "Evaluating class Truck (n=1672)\n",
      "  top-1: 0.010 (1.0%), top-2: 0.035 (3.5%), top-3: 0.076 (7.6%), top-4: 0.989 (98.9%), top-5: 1.000 (100.0%)\n",
      "Evaluating class Cyclist (n=11894)\n",
      "  top-1: 0.176 (17.6%), top-2: 0.691 (69.1%), top-3: 0.935 (93.5%), top-4: 0.989 (98.9%), top-5: 1.000 (100.0%)\n",
      "Evaluating class Pedestrian (n=17930)\n",
      "  top-1: 0.618 (61.8%), top-2: 0.896 (89.6%), top-3: 0.961 (96.1%), top-4: 0.994 (99.4%), top-5: 1.000 (100.0%)\n",
      "\n",
      "Overall:\n",
      "  top-1: 0.167 (16.7%), top-2: 0.377 (37.7%), top-3: 0.600 (60.0%), top-4: 0.811 (81.1%), top-5: 1.000 (100.0%)\n",
      "\n",
      "Overall if guessing randomly:\n",
      "  top-1: 0.200 (20.0%), top-2: 0.400 (40.0%), top-3: 0.600 (60.0%), top-4: 0.800 (80.0%), top-5: 1.000 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def compute_accuracy(obj_feats: torch.Tensor, tru_class_idx: int) -> Dict[str, float]:\n",
    "    logits_per_text, _ = logit_img_txt(obj_feats, cls_embeddings_pt, clip_model)\n",
    "    score_per_class = logits_per_text.softmax(0).T\n",
    "    accuracies = {}\n",
    "    for k in range(1, min(6, len(score_per_class))):\n",
    "        topk = (score_per_class.argsort(axis=1, descending=True)[:, :k] == tru_class_idx).sum() / len(score_per_class)\n",
    "        accuracies[f\"top-{k}\"] = topk\n",
    "    return accuracies\n",
    "\n",
    "overall = defaultdict(float)\n",
    "for class_name, cls_obj_feats in obj_feats.items():\n",
    "    print(\"Evaluating class\", class_name, f\"(n={len(cls_obj_feats)})\")\n",
    "    accuracies = compute_accuracy(cls_obj_feats, CATEGORIES.index(class_name))\n",
    "    res_string = \", \".join(f\"{k}: {v:.3f} ({v*100:.1f}%)\" for k, v in accuracies.items())\n",
    "    print(f\"  {res_string}\")\n",
    "    for k, v in accuracies.items():\n",
    "        overall[k] += v\n",
    "overall = {k: v / len(obj_feats) for k, v in overall.items()}\n",
    "res_string = \", \".join(f\"{k}: {v:.3f} ({v*100:.1f}%)\" for k, v in overall.items())\n",
    "print(f\"\\nOverall:\")\n",
    "print(f\"  {res_string}\")\n",
    "print(\"\\nOverall if guessing randomly:\")\n",
    "# Compute the probability of guessing correctly by chance\n",
    "rand_acc = 1 / len(CATEGORIES)\n",
    "res_string = \", \".join(f\"{k}: {v:.3f} ({v*100:.1f}%)\" for k, v in {k: rand_acc*int(k.split(\"-\")[1]) for k in overall}.items())\n",
    "print(f\"  {res_string}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8474fbf7fa6f299d9ca87dcd7358dfc28aa95d8ec78802489d98a6cd3ecc0cc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
